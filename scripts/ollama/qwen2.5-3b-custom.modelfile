# Qwen 2.5 3B Instruct - Optimiert für Hablará (Voice Intelligence Pipeline)
#
# Quality Tier: Lightweight / CPU-only
# Empfohlen für Systeme ohne GPU-Beschleunigung oder mit wenig RAM.
# 2-3x schneller als 7B, deutlich weniger RAM-Verbrauch.
#
# Optimierungen:
# - Context auf 8K limitiert (Hablará braucht max 1K tokens)
# - Temperature 0.3 für deterministische strukturierte Outputs
# - Top-p 0.9 für konsistente JSON-Generierung
# - Repeat Penalty gegen Halluzination-Loops
#
# Performance-Erwartung:
# - JSON Reliability: ~90% (etwas unter 7B)
# - Geschwindigkeit: 2-3x schneller als 7B
# - RAM: ~4 GB (vs. ~7 GB bei 7B)
#
# Usage:
#   ollama create qwen2.5:3b-custom -f qwen2.5-3b-custom.modelfile
#   ollama run qwen2.5:3b-custom

FROM qwen2.5:3b

# Context Window: 8K ausreichend für Hablará (max 650 tokens Prompt)
# Reduziert RAM-Verbrauch → +15% Speed
PARAMETER num_ctx 8192

# Temperature: 0.3 für strukturierte Outputs (JSON Schema)
# Niedrig = deterministisch, konsistent
PARAMETER temperature 0.3

# Top-p: 0.9 für Nucleus Sampling
# Balanciert Kreativität und Determinismus
PARAMETER top_p 0.9

# Repeat Penalty: Verhindert Halluzination-Loops
# Wichtig für JSON-Generierung (keine endlosen Arrays)
PARAMETER repeat_penalty 1.1

# Stop Sequences: Removed - verhinderte vollständiges JSON
# PARAMETER stop "}" ← PROBLEM: Stoppt BEVOR } geschrieben wird!
# num_predict in API-Call + format:"json" reichen aus

# System Message: Hablará-Context
SYSTEM """Du bist ein KI-Assistent für emotionale und argumentative Textanalyse.

Deine Aufgaben:
1. Emotion Analysis: Erkenne die primäre Emotion in gesprochenen Texten (Deutsch)
2. Fallacy Detection: Identifiziere logische Fehlschlüsse in Argumenten
3. JSON Output: Antworte IMMER in gültigem JSON-Format

Wichtig:
- Sei präzise und objektiv
- Berücksichtige deutschen Sprachgebrauch und Kultur
- Gib strukturierte Antworten (JSON Schema)
- Keine Halluzinationen oder erfundene Details
"""
