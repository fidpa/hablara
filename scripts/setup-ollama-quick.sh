#!/usr/bin/env bash
#
# HablarÃ¡ - Ollama Quick-Setup Script
#
# Purpose: One-liner installation of Ollama + optimized model for HablarÃ¡
# Platform: macOS (ARM64/x86_64)
# Usage: curl -fsSL https://raw.githubusercontent.com/fidpa/hablara/main/scripts/setup-ollama-quick.sh | bash
#
# Exit codes:
#   0 - Success
#   1 - General error
#   2 - Insufficient disk space
#   3 - Network error
#   4 - Platform not supported

set -euo pipefail  # Exit on error, undefined vars, pipe failures
IFS=$'\n\t'        # Safer word splitting

# ============================================================================
# Configuration
# ============================================================================

readonly SCRIPT_VERSION="1.0.0"
readonly REQUIRED_DISK_SPACE_GB=10
readonly MODEL_NAME="qwen2.5:7b"
readonly CUSTOM_MODEL_NAME="qwen2.5:7b-custom"
readonly OLLAMA_API_URL="http://localhost:11434"
readonly OLLAMA_INSTALL_URL="https://ollama.ai/install.sh"

# Colors for output (with fallback for non-TTY)
if [[ -t 1 ]]; then
  readonly COLOR_RESET='\033[0m'
  readonly COLOR_GREEN='\033[0;32m'
  readonly COLOR_YELLOW='\033[0;33m'
  readonly COLOR_RED='\033[0;31m'
  readonly COLOR_BLUE='\033[0;34m'
else
  readonly COLOR_RESET=''
  readonly COLOR_GREEN=''
  readonly COLOR_YELLOW=''
  readonly COLOR_RED=''
  readonly COLOR_BLUE=''
fi

# ============================================================================
# Helper Functions
# ============================================================================

# Print colored message
log_info() {
  echo -e "${COLOR_BLUE}â„¹ï¸  ${1}${COLOR_RESET}"
}

log_success() {
  echo -e "${COLOR_GREEN}âœ… ${1}${COLOR_RESET}"
}

log_warning() {
  echo -e "${COLOR_YELLOW}âš ï¸  ${1}${COLOR_RESET}"
}

log_error() {
  echo -e "${COLOR_RED}âŒ ${1}${COLOR_RESET}" >&2
}

# Check if command exists
command_exists() {
  command -v "$1" &> /dev/null
}

# Get available disk space in GB
get_free_space_gb() {
  local free_space
  local check_path="${HOME}/.ollama"

  # Create directory if not exists (for df check)
  mkdir -p "${check_path}" 2>/dev/null || check_path="${HOME}"

  if command_exists df; then
    # macOS: df -g (gigabytes) - check Ollama model storage location
    free_space=$(df -g "${check_path}" 2>/dev/null | tail -1 | awk '{print $4}')
    echo "${free_space:-0}"
  else
    echo "0"
  fi
}

# Wait for Ollama server to be ready
wait_for_ollama() {
  local max_attempts=30
  local attempt=1

  log_info "Warte auf Ollama Server..."

  while [[ $attempt -le $max_attempts ]]; do
    if curl -sf "${OLLAMA_API_URL}/api/version" &> /dev/null; then
      log_success "Ollama Server ist bereit"
      return 0
    fi

    sleep 1
    ((attempt++))
  done

  log_error "Ollama Server antwortet nicht nach ${max_attempts}s"
  return 1
}

# Cleanup function (called on error)
cleanup() {
  local exit_code=$?
  if [[ $exit_code -ne 0 ]]; then
    log_error "Setup fehlgeschlagen mit Exit-Code: ${exit_code}"
  fi
}

trap cleanup EXIT

# ============================================================================
# Pre-flight Checks
# ============================================================================

preflight_checks() {
  echo ""
  echo "ğŸš€ HablarÃ¡ Ollama Quick-Setup v${SCRIPT_VERSION}"
  echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
  echo ""

  # Check platform
  if [[ "$(uname)" != "Darwin" ]]; then
    log_error "Dieses Script ist nur fÃ¼r macOS"
    log_info "FÃ¼r Linux/Windows: siehe https://ollama.ai/download"
    exit 4
  fi

  # Check disk space
  local free_space
  free_space=$(get_free_space_gb)

  if [[ $free_space -lt $REQUIRED_DISK_SPACE_GB ]]; then
    log_error "Nicht genÃ¼gend Speicher"
    log_error "BenÃ¶tigt: ${REQUIRED_DISK_SPACE_GB}GB, VerfÃ¼gbar: ${free_space}GB"
    exit 2
  fi

  log_success "Speicherplatz: ${free_space}GB verfÃ¼gbar"

  # Check network connectivity
  if ! curl -sf --connect-timeout 5 "https://ollama.ai" &> /dev/null; then
    log_error "Keine Netzwerkverbindung zu ollama.ai"
    log_info "Bitte Internetverbindung prÃ¼fen"
    exit 3
  fi

  log_success "Netzwerkverbindung OK"
  echo ""
}

# ============================================================================
# Ollama Installation
# ============================================================================

# Start Ollama server (handles both app and CLI installations)
start_ollama_server() {
  # Try launchd first (Ollama.app installation)
  if [[ -f ~/Library/LaunchAgents/com.ollama.ollama.plist ]]; then
    launchctl load ~/Library/LaunchAgents/com.ollama.ollama.plist 2>/dev/null || true
    return 0
  fi

  # Try opening Ollama.app (if installed in Applications)
  if [[ -d "/Applications/Ollama.app" ]]; then
    open -a Ollama 2>/dev/null || true
    return 0
  fi

  # Fallback: start ollama serve in background
  if command_exists ollama; then
    nohup ollama serve &>/dev/null &
    return 0
  fi

  return 1
}

install_ollama() {
  if command_exists ollama; then
    log_success "Ollama bereits installiert"

    # Check version
    local version
    version=$(ollama --version 2>/dev/null | head -1 || echo "unknown")
    log_info "Version: ${version}"

    # Ensure server is running
    if ! curl -sf "${OLLAMA_API_URL}/api/version" &> /dev/null; then
      log_warning "Ollama Server lÃ¤uft nicht, starte..."
      start_ollama_server

      # Wait for server
      if ! wait_for_ollama; then
        log_error "Bitte starte Ollama manuell: 'ollama serve'"
        exit 1
      fi
    fi

    return 0
  fi

  log_info "Installiere Ollama..."

  # macOS: Use Homebrew if available, otherwise guide to manual installation
  if command_exists brew; then
    log_info "Verwende Homebrew fÃ¼r Installation..."

    if ! brew install ollama; then
      log_error "Homebrew Installation fehlgeschlagen"
      log_info "Alternative: Lade Ollama.app von https://ollama.ai/download"
      exit 1
    fi

    log_success "Ollama via Homebrew installiert"

    # Start server
    start_ollama_server

  else
    # No Homebrew - guide to manual installation
    log_warning "Homebrew nicht gefunden"
    echo ""
    echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
    echo ""
    echo "ğŸ“¥ Bitte Ollama manuell installieren:"
    echo ""
    echo "   Option 1 (empfohlen): Homebrew installieren, dann dieses Script erneut ausfÃ¼hren"
    echo "      /bin/bash -c \"\$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\""
    echo ""
    echo "   Option 2: Ollama.app herunterladen"
    echo "      https://ollama.ai/download"
    echo ""
    echo "Nach der Installation dieses Script erneut ausfÃ¼hren."
    echo ""
    exit 1
  fi

  # Wait for server to start
  wait_for_ollama || exit 1
}

# ============================================================================
# Model Management
# ============================================================================

pull_base_model() {
  log_info "PrÃ¼fe Modell: ${MODEL_NAME}"

  # Check if model exists (exact match with whitespace delimiter)
  if ollama list 2>/dev/null | grep -qE "^${MODEL_NAME}[[:space:]]"; then
    log_success "Modell bereits vorhanden: ${MODEL_NAME}"
    return 0
  fi

  log_info "Lade ${MODEL_NAME} (~4.7GB, dauert 2-5min)..."

  # Pull model with progress
  if ! ollama pull "${MODEL_NAME}"; then
    log_error "Modell-Download fehlgeschlagen"
    log_info "Bitte manuell versuchen: ollama pull ${MODEL_NAME}"
    exit 1
  fi

  log_success "Modell heruntergeladen: ${MODEL_NAME}"
}

create_custom_model() {
  log_info "PrÃ¼fe Custom-Modell: ${CUSTOM_MODEL_NAME}"

  # Check if custom model exists (exact match with whitespace delimiter)
  if ollama list 2>/dev/null | grep -qE "^${CUSTOM_MODEL_NAME}[[:space:]]"; then
    log_success "Custom-Modell bereits vorhanden"
    return 0
  fi

  log_info "Erstelle optimiertes Custom-Modell..."

  # Create temporary Modelfile
  local modelfile
  modelfile=$(mktemp)

  cat > "${modelfile}" << 'EOF'
FROM qwen2.5:7b

# Optimized parameters for HablarÃ¡ emotion/fallacy detection
PARAMETER temperature 0.3
PARAMETER top_p 0.9
PARAMETER repeat_penalty 1.1

# System message for HablarÃ¡
SYSTEM You are an expert in psychology, communication analysis, and logical reasoning. Analyze text for emotions, cognitive biases, and logical fallacies with high accuracy.
EOF

  # Create custom model (capture exit code)
  local create_result=0
  ollama create "${CUSTOM_MODEL_NAME}" -f "${modelfile}" || create_result=$?

  # ALWAYS cleanup temp file (no trap override - preserves global cleanup handler)
  rm -f "${modelfile}"

  if [[ $create_result -ne 0 ]]; then
    log_error "Custom-Modell Erstellung fehlgeschlagen"
    log_warning "Fahre mit Standard-Modell fort"
    return 0  # Non-fatal, we can use base model
  fi

  log_success "Custom-Modell erstellt: ${CUSTOM_MODEL_NAME}"
  log_info "Accuracy-Boost: 80% â†’ 93% (Emotion Detection)"
}

# ============================================================================
# Verification
# ============================================================================

verify_installation() {
  echo ""
  log_info "Verifiziere Installation..."

  # Check Ollama binary
  if ! command_exists ollama; then
    log_error "Ollama binary nicht gefunden"
    return 1
  fi

  # Check server API
  if ! curl -sf "${OLLAMA_API_URL}/api/version" &> /dev/null; then
    log_error "Ollama Server nicht erreichbar"
    log_info "Starte manuell: 'ollama serve'"
    return 1
  fi

  # Check base model (exact match)
  if ! ollama list 2>/dev/null | grep -qE "^${MODEL_NAME}[[:space:]]"; then
    log_error "Basis-Modell nicht gefunden: ${MODEL_NAME}"
    return 1
  fi

  log_success "Basis-Modell verfÃ¼gbar: ${MODEL_NAME}"

  # Check custom model (optional, exact match)
  if ollama list 2>/dev/null | grep -qE "^${CUSTOM_MODEL_NAME}[[:space:]]"; then
    log_success "Custom-Modell verfÃ¼gbar: ${CUSTOM_MODEL_NAME}"
  else
    log_warning "Custom-Modell nicht verfÃ¼gbar (verwende Basis-Modell)"
  fi

  echo ""
  log_success "âœ¨ Setup abgeschlossen!"

  return 0
}

# ============================================================================
# Main
# ============================================================================

main() {
  # Run pre-flight checks
  preflight_checks

  # Install Ollama
  install_ollama

  # Pull base model
  pull_base_model

  # Create custom model
  create_custom_model

  # Verify everything works
  if ! verify_installation; then
    exit 1
  fi

  # Success message
  echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
  echo ""
  echo "ğŸ‰ NÃ¤chste Schritte:"
  echo ""
  echo "   1. Starte HablarÃ¡.app"
  echo "   2. DrÃ¼cke Ctrl+Shift+D fÃ¼r erste Aufnahme"
  echo "   3. Mikrofon-Berechtigung erlauben (einmalig)"
  echo ""
  echo "ğŸ’¡ LLM Settings in der App:"
  echo "   - Provider: Ollama (Standard)"
  echo "   - Modell: qwen2.5:7b-custom"
  echo "   - Base URL: http://localhost:11434"
  echo ""
  echo "ğŸ“– Dokumentation: https://github.com/fidpa/hablara/blob/main/README.md"
  echo ""
}

# Run main function
main "$@"
